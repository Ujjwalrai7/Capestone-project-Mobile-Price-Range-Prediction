{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "qjKvONjwE8ra",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ujjwalrai7/Capestone-project-Mobile-Price-Range-Prediction/blob/main/Capestone_Project_mobile_price_range_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Mobile Price Range Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification Project\n",
        "##### **Contribution**    - Individual Project By Rai Ujjwal Manoj"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mobile now a days is one of the most selling and purchasing device. Every day new mobiles with new version and more features are launched. Hundreds and thousands of mobiles are sold and purchased on daily basis.\n",
        "\n",
        "Therefore, Price estimation and prediction is an important part of consumer strategy as a new product that has to be launched, must have the correct price so that the consumers find it appropriate to buy the product. During the purchase of mobile phones, people fail to make correct decisions due to the non-availability of necessary resources to cross validate the price. To address this issue, we developed different classification models using the data related to different features of a mobile phone. The developed model is then used to predict the price range of the new mobile phones.\n",
        "\n",
        "The main objective of this work is to find out the relationship between features of a mobile phone and its price range which indicates whether the mobile would be cheap(0), mid-range(1), expensive(2) or very expensive(3). We will be using different classification models to accurately classify the data in correct price ranges.\n",
        "\n",
        "Based on our dataset, Data prepressing was the first step followed. I have understood the data found that the dataset contains 2000 records of mobile phone information with 21 features which were a mix of categorical and numerical values also the dataset fortunately is not having any null values, got a clear description about the features involved. After that Exploratory Data Analysis and Data Visualization has provided a brief understanding about the relationship present between features and label i.e., the dependent variable also it has given an idea about the features to be selected for the further process.\n",
        "\n",
        "Heatmap was used to understand the correlation between independent variables, based on which important features were selected. Before fitting the model, Standardization was an important step, it makes the feature values in the data have zero mean and unit variance. While we implement any Machine Learning algorithm it could be a possibility that objective function will not work properly without normalization.\n",
        "\n",
        "After training and testing was done. I have made the use of Logistic Regression, K Nearest Neighbour, Random Forest, XGBoost, Support Vector Machine techniques.Hyperparameter Tuning was also done over some of the models for better accuracy and to reduce overfitting, made the use of Gridsearch cross validation to achieve the best parameter. These parameters enhanced the predicting capability of our model.\n",
        "\n",
        "Checked and compared various matrices and came to conclusion that XGBoost model with hyperparameter tuning is giving the best scores. Also, Tree based models (XGBoost and Random Forest in our case) are by far good performing models while dealing with our dataset because of its ability to stay insulated from the effect of worst performing features and at the same time KNN proven to be the worst for this dataset."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Ujjwalrai7/Capestone-project-Mobile-Price-Range-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### In the competitive mobile phone market companies want to understand sales data of mobile phones and factors which drive the prices.\n",
        "\n",
        "### The objective is to find out some relation between features of a mobile phone(e.g:- RAM, Internal Memory etc) and its selling price. **In this problem, we do not have to predict the actual price but a price range indicating how high the price is.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the required libraries\n",
        "import warnings                                        # do not disturb mode\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np                                     # numerical computations\n",
        "import pandas as pd                                    # data manipulations\n",
        "\n",
        "import matplotlib.pyplot as plt                        # visualize with plots\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-white')\n",
        "import plotly.express as px\n",
        "\n",
        "from datetime import datetime                          # to convert to date\n",
        "from statsmodels.tsa.arima_model import ARIMA          # to build ARIMA\n",
        "\n",
        "\n",
        "from dateutil.relativedelta import relativedelta       # working with dates with style\n",
        "from datetime import datetime                          # computational cost\n",
        "from scipy.optimize import minimize                    # for function minimization\n",
        "import copy                                            # create copies\n",
        "\n",
        "from sklearn.preprocessing import (MinMaxScaler,       # scale the data\n",
        "StandardScaler)\n",
        "from sklearn.model_selection import train_test_split   # split train and test data\n",
        "from sklearn.model_selection import (cross_val_score,  # split train and test data on a timeseries\n",
        "TimeSeriesSplit)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, auc\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller         # statistics and econometrics\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tsa.api as smt\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs"
      ],
      "metadata": {
        "id": "3uc_NCaMbz6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "vj-kJ5OUQxbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset=pd.read_csv(\"/content/drive/MyDrive/data_mobile_price_range.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Data is duplicated ? {dataset.duplicated().value_counts()},unique values with {len(dataset[dataset.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "-EIuCE6PX4w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset[dataset.duplicated()])"
      ],
      "metadata": {
        "id": "VuQvlviCYEom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are 21 columns in the dataset including the target variable.\n",
        "\n",
        "* There are a total of 2000 records of mobile phone data in our dataset.\n",
        "\n",
        "* The dataset doesn't contain any null values as per the info()."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Battery_power:** Total energy a battery can store in one time measured in mAh\n",
        "\n",
        "#### **Blue:** Has bluetooth or not\n",
        "\n",
        "#### **Clock_speed:** Speed at which microprocessor executes instructions\n",
        "\n",
        "#### **Dual_sim:** Has dual sim support or not\n",
        "\n",
        "#### **Fc:** Front camera mega pixels\n",
        "\n",
        "#### **Four_g:** Has 4G or not\n",
        "\n",
        "#### **Int_memory:** Internal memory in gigabytes\n",
        "\n",
        "#### **M_dep:** Mobile depth in cm\n",
        "\n",
        "#### **Mobile_wt:** Weight of mobile phone\n",
        "\n",
        "#### **N_cores:** Number of cores of processor\n",
        "\n",
        "#### **Pc:** Primary camera mega pixels\n",
        "\n",
        "#### **Px_height:** Pixel resolution height\n",
        "\n",
        "#### **Px_width:** Pixel resolution width\n",
        "\n",
        "#### **Ram:** Random Access Memory in Megabytes\n",
        "\n",
        "#### **Sc_h:** Screen height of mobile in cm\n",
        "\n",
        "#### **Sc_w:** Screen width of mobile in cm\n",
        "\n",
        "#### **Talk_time:** Longest time that a single battery charge will last when you are talking over phone\n",
        "\n",
        "#### **Three_g:** Has 3G or not\n",
        "\n",
        "#### **Touch_screen:** Has touch screen or not\n",
        "\n",
        "#### **Wifi:** Has wifi or not\n",
        "\n",
        "#### **Price_range:** This is the target variable with value of **0(low cost)**, **1(medium cost)**, **2(high cost)** and **3(very high cost)**."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in dataset.columns.tolist():\n",
        "  print(f\"No. of unique values in {i} is {dataset[i].nunique()}.\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "DJYdia6ZnMd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# converting the columns to relevent names\n",
        "dataset.rename(columns = {\"blue\":\"bluetooth\", \"fc\": \"front_camera\" ,\"m_dep\":\"mobile_depth\",\"pc\":\"primary_camera\",\"n_cores\":\"no_of_cores\" ,\"sc_h\":\"screen_height\",\"sc_w\":\"screen_width\"},inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "2YRV7WVRNICj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking value counts of the dependent Variable\n",
        "dataset_VC=dataset[\"price_range\"].value_counts()\n",
        "dataset_VC.reset_index()"
      ],
      "metadata": {
        "id": "UbZW-EEWlQjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mean value of ram w.r.t price range\n",
        "dataset_ram=dataset.groupby(\"price_range\")[\"ram\"].mean().reset_index()\n",
        "dataset_ram\n"
      ],
      "metadata": {
        "id": "LXHMuJOxteTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mean value of battery power w.r.t price_range\n",
        "dataset_battery=dataset.groupby('price_range')[\"battery_power\"].mean().reset_index()\n",
        "dataset_battery"
      ],
      "metadata": {
        "id": "boaawaYSub9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mean value of primary camera w.r.t price range\n",
        "dataset_pc=dataset.groupby(\"price_range\")[\"primary_camera\"].mean().reset_index()\n",
        "dataset_pc"
      ],
      "metadata": {
        "id": "P3m8IaAou7OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The dataset contains 2000 records of mobile phone information with 21 features. There are 6 categorical features and 14 numerical features with price range as the target variable.\n",
        "\n",
        "* The dataset was almost a cleaned one with no null values present or duplicate records found.\n",
        "\n",
        " * Battery power and RAM increases with the increase in price range and thus these two features will be an influential factor for predicting the price range.\n",
        "\n",
        "  * Most of the other numerical features doesn't show a significant change with the price range.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Univariate Analysis"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "dataset[\"price_range\"].value_counts().plot(kind=\"pie\",autopct=\"%1.1f%%\")"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oM53w4LJwvL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the distribution of Price range on various categories"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that can be generated are that they price range is almost evenly distributed in all the categories."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bivariate Analysis"
      ],
      "metadata": {
        "id": "da_pIdOasJB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Price Range vs RAM"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.barplot(x='price_range', y='ram', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the dependency of price range on feature RAM of mobile.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The in sights that can be generated are that price range is highly dependent on RAM of mobile and increases according to the increase in the RAM of the mobile."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.lineplot(x='price_range', y='battery_power', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the change in range of price with the change in battery power of mobile."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the battery power capacity for mobiles in 1 and 2 almost same while the cheapest phones have the lowest battery power and and the hihly priced mobile have high battery powers."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Price range vs Internal Memory"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.boxplot(x='price_range', y='int_memory', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UsqwKn9QoFY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the change in price range with the change in internal memory."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price Range does not show any significant change with the change in internal Memory."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.lineplot(x='price_range', y='primary_camera', data=dataset)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Svh2yz88ymw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the dependency of Price range on Primary camera feature."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price range is significantly dependent on primary camera."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plotting the piecharts for categorical variables.\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "rows=2\n",
        "col=3\n",
        "count=1\n",
        "var_list=['bluetooth','dual_sim','four_g','three_g','wifi','touch_screen']\n",
        "labels=['Yes','No']\n",
        "for var in var_list:\n",
        "  plt.subplot(rows,col,count)\n",
        "\n",
        "  dataset[var].value_counts().plot.pie(autopct='%1.1f%%',fontsize=12,figsize=(12,10),labels=labels)\n",
        "  plt.title(f'Quanity of {var}',fontsize=14)\n",
        "  plt.show()\n",
        "  count=count+1"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the distribution of various categorical columns into different columns."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all the columns are evenly distributed but it can be seen that the mobiles with 3G connection are comparatively high in numbers compared to mobile phones without 3G connections."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate Analysis"
      ],
      "metadata": {
        "id": "5IRAu9E5634j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Phones of various price range supporting important features or not"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1=dataset.groupby(['price_range']).agg({'bluetooth':\"value_counts\",\"dual_sim\":\"value_counts\",\"four_g\":\"value_counts\",\"three_g\":\"value_counts\",\"wifi\":\"value_counts\",\"touch_screen\":\"value_counts\"}).unstack()\n",
        "df_1"
      ],
      "metadata": {
        "id": "1qWgZ6ypq2ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# plotting barplot to know that how much price get affected by following features\n",
        "df_1.plot.bar(figsize=(28,16))\n",
        "plt.title('NUMBER OF PHONES SUPORTED FOLLOWING SPECIFICATIONS OR NOT')\n",
        "plt.xlabel('PHONE COST')\n",
        "plt.ylabel('NUMBER OF PHONES')"
      ],
      "metadata": {
        "id": "JW8QHo6NuwMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the features of most of the phones in various Price Range"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all the features are available in each price range of mobile from low to high category. Also phones with 3G connectivity are more compared to 4G connectivity in each Price Range."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact.\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Let us find out how number of cores and RAM vary with price ?"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.barplot(x='no_of_cores', y='ram', hue='price_range', data=dataset)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L0YKgvBlvyHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "find out how number of cores and battery power vary with price"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can observe that for very high cost category we get the maximum RAM across all number of cores available.\n",
        "\n",
        "* Hence, RAM is a strong predictor of the price range as we can see RAM increases with price for each number of cores available."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Let us find out how number of cores and Screen Height vary with price ?"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Cores wise cost of mobile phone\n",
        "specs=['no_of_cores','screen_height']\n",
        "for item in specs:\n",
        "\n",
        "  dataset.groupby(['price_range'])[item].value_counts().unstack().plot.bar(figsize=(20,10))\n",
        "  plt.title(f'Price range grouped by {item}')\n",
        "  plt.ylabel('No. of phones')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " To find out how number of cores and Screen Height vary with price ?"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SO BY ABOVE GRAPHS WE CAN CLEARLY SAY THAT LESS THICK PHONES ARE HIGH COMPARING TO HIGH THCKNESS AND MAXIMUMUM MOBILES IN PRICE RANGE 2 & 3 HAVE MORE NO OF CORES."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gain insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Visualisation of outliers"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "for col in dataset.columns:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  sns.boxplot(dataset[col])\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if there are any outliers present in the features or not."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The boxplots suggest us that there are outliers in frontcamera and px_height but we won't treat any value as outlier here because these values are possible realistically to have in different kinds of mobiles available in the market.\n",
        "\n",
        "* We know that there are phones which have 20MP or more as front camera so it is possible to have values upto 17.5MP as front camera.\n",
        "\n",
        "* We have even seen that there are big sized mobile phones which require more resolution available having 4K, pure HD screens etc."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Visualizing the correlation of all the features with the target variable"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "for col in dataset.columns[ :-1] :\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = dataset[col]\n",
        "    label = dataset['price_range']\n",
        "    correlation = feature.corr(label)\n",
        "    plt.scatter(x=feature, y=label)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('price_range')\n",
        "    ax.set_title('price_range vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(dataset[col], dataset['price_range'], 1)\n",
        "    y_hat = np.poly1d(z)(dataset[col])\n",
        "\n",
        "    plt.plot(dataset[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Visualize the correlation of all the features with the target variable"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the correlation plots above we can see that,\n",
        "\n",
        "* Only RAM is strongly correlated with the target variable. RAM will have a strong influence in determining the price ranges.\n",
        "\n",
        "* Battery Power, px_height and px_width are slightly correlated with the target variable and rest all of the features are not correlated or have very low correlation with the target variable.\n",
        "\n",
        "* From the correlation plots above, we can also see that no categorical feature is strongly correlated with the target variable.\n",
        "\n",
        "* All of the features have very low correlation with the target variable and interestingly touch_screen has a very low negative correlation with the target variable."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12  Distribution Plot"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        " #Plotting the distributions of all features.\n",
        "for col in dataset.columns:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(dataset[col], color='y')\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(dataset[col].mean(),color='green',linewidth=2)                            # axvline plots a vertical line at a value (mean in this case).\n",
        "  plt.axvline(dataset[col].median(),color='red',linestyle='dashed',linewidth=1.5)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the distribution of each feature."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plots above we can conclude that,\n",
        "\n",
        "* Many features have an almost uniform distribution which means there are products available almost equally in all sizes/values.\n",
        "\n",
        "* Few features like fc, px_height and sc_w are right skewed means there are more number of products listed with the lesser size/value available."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        " ## Correlation\n",
        "plt.figure(figsize=(15,8))\n",
        "correlation = dataset.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the correlation of each feature."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the correlation plot above we can observe that,\n",
        "\n",
        "* RAM is the only feature strongly correlated with the target variable which means RAM is a strong predictor for the price range variable.\n",
        "\n",
        "* fc and pc are correlated with a value of 0.65 which is obvious as primary camera increases front camera also increases. We cannot combine these features as both of these are different parameters while buying a mobile.\n",
        "\n",
        "* three_g and four_g are correlated with a value of 0.59 which is again obvious as whichever mobile supports 4G supports 3G too but not vice versa.\n",
        "\n",
        "* Most of the independent variables are not strongly correlated to each other, hence we do not need to eliminate any feature."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "d72dnMffJMxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the gained insights would help in creating a positive business impact."
      ],
      "metadata": {
        "id": "3077t-_TJTqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(dataset ,hue=\"price_range\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above chart depicts the correlation of various labels with respect to price range around."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(dataset.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, there were no missing values to handle in the given dataset no further manipulation was needed."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#visualization code\n",
        "for col in dataset.columns:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  sns.boxplot(dataset[col])\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The boxplots suggest us that there are outliers in frontcamera and px_height but we won't treat any value as outlier here because these values are possible realistically to have in different kinds of mobiles available in the market.\n",
        "\n",
        "* We know that there are phones which have 20MP or more as front camera so it is possible to have values upto 17.5MP as front camera.\n",
        "\n",
        "* We have even seen that there are big sized mobile phones which require more resolution available having 4K, pure HD screens etc."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not needed in this Case."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features.# Correlation Heatmap visualization code\n",
        " ## Correlation\n",
        "plt.figure(figsize=(15,8))\n",
        "correlation = dataset.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we can see that there is very less correlation between price range with no_of_cores, clock_speed and mobile_depth  of the Mobile. So we can drop these features so as to get better results in this model."
      ],
      "metadata": {
        "id": "M5PWSRlYCgUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "mobile_df=dataset.copy()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping some unwanted feature\n",
        "mobile_df.drop(['no_of_cores','clock_speed','mobile_depth','mobile_wt','touch_screen'],axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "EyMF_te4DS6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging Screen length and Screen width into screen Area\n",
        "mobile_df['screen_area'] = mobile_df['screen_height'] * mobile_df['screen_width']\n",
        "mobile_df.drop(['screen_height','screen_width'],axis=1, inplace = True)\n",
        "mobile_df.head()"
      ],
      "metadata": {
        "id": "F4abc0CWD8YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIF**\n",
        "\n",
        "---\n",
        "* Calculating vif identify the strength of correlation between independent variable and the strength of that correlation.\n",
        "\n",
        "* Vif starts at 1 and has no upper limit.\n",
        "\n",
        "* 1-5 : Moderate, No corrective measure\n",
        "\n",
        "* Greater than 5: Severe, Coefficient and p-value are questionable\n"
      ],
      "metadata": {
        "id": "2e7sFiFrrK-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "   # Calculating VIF\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "   return(vif)"
      ],
      "metadata": {
        "id": "Updg4YdYOfJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(mobile_df[[i for i in mobile_df.describe().columns if i not in ['price_range'] ]])"
      ],
      "metadata": {
        "id": "E4ghMaabO0WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all the vif values are less than 10 so this columns can be considered important for model fitting."
      ],
      "metadata": {
        "id": "5jJ9brWKPPoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used correlation heatmap for checking the collinearity of the independent variable also checked the vif score of the variables."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in the Distribution plot that most of the important features are normally distributed so the data need not to be transformed."
      ],
      "metadata": {
        "id": "VJQd2xGzP5CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset doesnot need any dimensionality reduction.\n",
        "\n",
        "Dimensionality reduction is a technique that is used to reduce the number of features in a dataset. It is often used when the number of features is very large, as this can lead to problems such as overfitting and slow computation. There are a variety of techniques that can be used for dimensionality reduction, such as principal component analysis (PCA) and singular value decomposition (SVD).\n",
        "\n",
        "There are several reasons why dimensionality reduction might be useful. One reason is that it can help to reduce the size of a dataset, which can be particularly useful when the dataset is very large. It can also help to improve the performance of machine learning models by reducing the number of features that the model has to consider, which can lead to faster computation and better generalization to new data.\n",
        "\n",
        "Another reason to use dimensionality reduction is to reduce the curse of dimensionality, which refers to the fact that as the number of dimensions increases, the volume of the space increases exponentially. This can lead to problems such as the nearest neighbor search becoming less effective, as the distances between points become much larger. Dimensionality reduction can help to reduce the curse of dimensionality by reducing the number of dimensions in the data.\n",
        "\n",
        "Finally, dimensionality reduction can also be useful for visualizing high-dimensional data. It can be difficult to visualize data in more than three dimensions, so reducing the number of dimensions can make it easier to understand the patterns in the data."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x = mobile_df.drop(columns=['price_range'], axis=1)\n",
        "y = mobile_df['price_range']\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=10)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case I have used 80:20 split as splitting in any ratio 80/20 or 70/30 deos not affect unless there is less values in dataset. However, there are two competing concerns: with less training data, parameter estimates have greater variance. With less testing data, performance statistic will have greater variance. it is best to divide data such that neither the variance in the training set nor the variance in the test set is too high.\n",
        "\n",
        "Example : If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
        "\n",
        "80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# standardizing the independent variables\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler =MaxAbsScaler()\n",
        "# fit the scaler to the train set, it will learn the parameters\n",
        "scaler.fit(x_train)\n",
        "\n",
        "# transform train and test sets\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are using an algorithm that assumes your features have a similar range, we should use feature scaling.\n",
        "\n",
        "If the ranges of features differ much then we should use feature scaling. If the range does not vary a lot like one of them is between 0 and 2 and the other one is between -1 and 0.5 then we can leave them as it's. However, we should use feature scaling if the ranges are, for example, between -2 and 2 and between -100 and 100.\n",
        "\n",
        "Use Standardization when your data follows Gaussian distribution. Use Normalization when your data does not follow Gaussian distribution.\n",
        "\n",
        "So, in my data few of the features were having large difference in distribution, that's why, I have used standardization using MaxAbsScaler on the dataset."
      ],
      "metadata": {
        "id": "WFoeXFc2U-Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "Imbalance means that the number of data points available for different the classes is different: If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our case it is not required and imbalanced as the target variable has 25% of observations of each class also  we have seen that almost all features have equal no.of.observations of each category. so class imbalance is generally not a concern."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the models and metrics\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix , precision_score, recall_score, f1_score , roc_auc_score , roc_curve"
      ],
      "metadata": {
        "id": "NVVSJSPuZU6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Logistic Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "clf.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lr=clf.predict(x_train)\n",
        "y_pred_lr=clf.predict(x_test)"
      ],
      "metadata": {
        "id": "XDNtZuGfMvWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix , precision_score, recall_score, f1_score , roc_auc_score , roc_curve"
      ],
      "metadata": {
        "id": "AjQlGv6QbJrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list of matrix to store the evaluation matrix of all model\n",
        "Train_acc=[]\n",
        "Train_precision=[]\n",
        "Train_recall=[]\n",
        "Train_f1=[]\n",
        "Train_roc=[]\n",
        "Test_acc=[]\n",
        "Test_precision=[]\n",
        "Test_recall=[]\n",
        "Test_f1=[]\n",
        "Test_roc=[]"
      ],
      "metadata": {
        "id": "MKxYiU9o9cBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making a function to train and evaluate classification model\n",
        "def train_model (model,x_train,y_train,x_test,y_test):\n",
        "\n",
        "#training the model\n",
        "  model.fit(x_train,y_train)\n",
        "# calculating accuracy on train and test set\n",
        "\n",
        "  #predicting the values\n",
        "  pred_train = model.predict(x_train)\n",
        "  pred = model.predict(x_test)\n",
        "  pred_train_prob = model.predict_proba(x_train)\n",
        "  pred_prob = model.predict_proba(x_test)\n",
        "\n",
        "\n",
        "\n",
        "  print('\\n================Trainining Evalution Matrix=========================\\n')\n",
        "\n",
        "  #finding training accuracy\n",
        "  train_acc = accuracy_score(y_train, pred_train)\n",
        "  print(\"Train_acc :\" , train_acc)\n",
        "\n",
        "  #finding training precision\n",
        "  train_precision = precision_score(y_train, pred_train, average='weighted')\n",
        "  print(\"Train_precision :\" ,train_precision)\n",
        "\n",
        "  #finding training recall\n",
        "  train_recall = recall_score(y_train, pred_train, average='weighted')\n",
        "  print(\"Train_recall:\" ,train_recall)\n",
        "  #finding training f1 score\n",
        "  train_f1 = f1_score(y_train, pred_train, average ='weighted')\n",
        "  print(\"Train_f1: \",train_f1 )\n",
        "\n",
        "  # calculating roc_auc_score on the train set\n",
        "  train_roc = roc_auc_score(y_train, pred_train_prob, multi_class='ovo', average='weighted')\n",
        "  print(\"Train_roc\", train_roc)\n",
        "\n",
        "\n",
        "  print('\\n================Test Evalution Matrix=========================\\n')\n",
        "\n",
        "  #finding test accuracy\n",
        "  test_acc = accuracy_score(y_test, pred)\n",
        "  print(\"Test_acc :\" , test_acc)\n",
        "\n",
        "  #finding training precision\n",
        "  test_precision = precision_score(y_test, pred, average='weighted')\n",
        "  print(\"Test_precision :\" ,test_precision)\n",
        "\n",
        "  #finding training recall\n",
        "  test_recall = recall_score(y_test, pred, average='weighted')\n",
        "  print(\"Test_recall:\" ,test_recall)\n",
        "  #finding training f1 score\n",
        "  test_f1 = f1_score(y_test, pred, average ='weighted')\n",
        "  print(\"Test_f1: \",test_f1 )\n",
        "\n",
        "  # calculating roc_auc_score on the train set\n",
        "  test_roc = roc_auc_score(y_test, pred_prob, multi_class='ovo', average='weighted')\n",
        "  print(\"Test_roc\", test_roc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#appending metrice to list\n",
        "  #mean_sq_error.append(MSE)\n",
        "  #root_mean_sq_error.append(RMSE)\n",
        "  #r2_list.append(r2)\n",
        "  #adj_r2_list.append(adj_r2)\n",
        "  Train_acc.append(train_acc)\n",
        "  Train_precision.append(train_precision)\n",
        "  Train_recall.append(train_recall)\n",
        "  Train_f1.append(train_f1)\n",
        "  Train_roc.append(train_roc)\n",
        "  Test_acc.append(test_acc)\n",
        "  Test_precision.append(test_precision)\n",
        "  Test_recall.append(test_recall)\n",
        "  Test_f1.append(test_f1)\n",
        "  Test_roc.append(test_roc)\n"
      ],
      "metadata": {
        "id": "ShhJ8Gn-9iQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# calling train_model to train,fit and evalution of logistic regression model\n",
        "train_model(clf,x_train,y_train,x_test,y_test)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for confusion matrix\n",
        "def matrix(actual, predicted):\n",
        "  labels = ['0','1','2','3']\n",
        "  cm = confusion_matrix(actual, predicted)\n",
        "  print(cm)\n",
        "\n",
        "  ax = plt.subplot()\n",
        "  sns.heatmap(cm, annot=True, ax=ax)\n",
        "\n",
        "  ax.set_xlabel('Predicted labels')\n",
        "  ax.set_ylabel('Actual labels')\n",
        "  ax.set_title('Confusion Matrix')\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "ZJXcMmoIcaCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lr=clf.predict(x_train)\n",
        "y_pred_lr=clf.predict(x_test)"
      ],
      "metadata": {
        "id": "mcAx-JZwNIrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "GHs8NdSDjht3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.regression.linear_model import pred\n",
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_lr)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test, y_pred_lr)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_lr))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "-0_dGj1GeJVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC Curve"
      ],
      "metadata": {
        "id": "wN895tLijms9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_pred_proba = clf.predict_proba(x_test)\n",
        "roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "tym5xCrmbZTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Random Forest"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "model_rf = RandomForestClassifier().fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_rf=model_rf.predict(x_train)\n",
        "y_pred_test_rf =model_rf.predict(x_test)"
      ],
      "metadata": {
        "id": "pLyvzvYXhIUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# calling train_model to train,fit and evalution of random forest model\n",
        "train_model(model_rf,x_train,y_train,x_test,y_test)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "ALb3cW8Njr0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.regression.linear_model import pred\n",
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_rf)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test, y_pred_test_rf)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_test_rf))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "fyyqFo5jN_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(estimator=rf_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2 )\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_grid.fit(x_train,y_train)\n",
        "forest = rf_grid.best_estimator_\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rf_grid=forest.predict(x_train)\n",
        "test_rf_grid = forest.predict(x_test)"
      ],
      "metadata": {
        "id": "8sVhSUs8INed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling train_model to train,fit and evalution of random forest with grid search cv model\n",
        "train_model(forest,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "5-FqyRhdJBx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "HTEPrXttjzOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, train_rf_grid)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test, test_rf_grid)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,test_rf_grid))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "eBIrcaUrftiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # ROC Curve"
      ],
      "metadata": {
        "id": "MWEvtgyXj3hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_pred_proba = forest.predict_proba(x_test)\n",
        "roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "ooysIDW3gHC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No I Haven't seen any significant improvement instead the testing accuracy has been dropped from 0.89 to 0.8."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics are used to assess the performance of machine learning models.\n",
        "\n",
        "1) Accuracy: Accuracy measures the proportion of correctly predicted instances out of the total instances. It indicates the overall correctness of the model's predictions. Higher accuracy generally suggests better performance. From a business perspective, accuracy helps in understanding the general reliability of the model's predictions. For example, in a spam email classification model, high accuracy means fewer legitimate emails are incorrectly classified as spam, reducing the risk of missing important messages.\n",
        "\n",
        "2) Precision: Precision represents the proportion of true positives (correctly predicted positive instances) out of all instances predicted as positive. Precision focuses on the accuracy of positive predictions and is particularly useful when the cost of false positives is high. In business scenarios, precision is essential when the model needs to minimize false positive predictions. For instance, in a credit fraud detection system, high precision reduces the number of false alarms, saving time and resources spent investigating false fraud cases.\n",
        "\n",
        "3) Recall (Sensitivity or True Positive Rate): Recall measures the proportion of true positives predicted by the model out of all actual positive instances. Recall is important when the cost of false negatives is high, i.e., when failing to detect positive instances has severe consequences. For example, in a medical diagnosis model, high recall ensures that a higher number of actual cases are correctly identified, preventing potential misdiagnoses and ensuring appropriate medical attention.\n",
        "\n",
        "4) F1 Score: F1 score combines precision and recall into a single metric by taking their harmonic mean. It provides a balance between precision and recall, making it useful when both false positives and false negatives need to be minimized. The F1 score is commonly used when there is an imbalance in class distribution or when achieving a balance between precision and recall is crucial for the business.\n",
        "\n",
        "5) ROC Curve and AUC: Receiver Operating Characteristic (ROC) curve plots the true positive rate (recall) against the false positive rate (1 - specificity) at various classification thresholds. Area Under the ROC Curve (AUC) summarizes the overall performance of the model across all possible thresholds. ROC curve and AUC are useful for evaluating the trade-off between true positive rate and false positive rate and help identify the optimal threshold for classification. From a business perspective, ROC curve and AUC provide insights into the model's ability to discriminate between classes, enabling informed decisions about the desired balance between true positives and false positives.\n",
        "\n",
        "The business impact of a classification ML model depends on how well it performs on these evaluation metrics. A model with high accuracy, precision, recall, and specificity can have a positive impact on business operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Extreme Gradient Boosting."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "#importing XGBoost Regressor\n",
        "from xgboost import XGBClassifier\n",
        "#creating Xgboost model\n",
        "xgb_model=XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb_model.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_train_xgb = xgb_model.predict(x_train)\n",
        "y_pred_xgb = xgb_model.predict(x_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# calling train_model to train,fit and evalution of XGBoost model\n",
        "train_model(xgb_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "QY31mR38j-K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_xgb)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test, y_pred_xgb)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_xgb))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "qgGY4Wk5PjtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost ROC CURVE"
      ],
      "metadata": {
        "id": "HZ_jXSBjHSs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_pred_proba = xgb_model.predict_proba(x_test)\n",
        "roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "jE_hTyPNGxqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV.)\n",
        "#creating param dict for gridsearch\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "Grid = GridSearchCV(GradientBoostingClassifier(), param_dict)\n",
        "Grid.fit(x_train,y_train)\n",
        "boosting = Grid.best_estimator_\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_train_xgb_cv = Grid.predict(x_train)\n",
        "y_pred_xgb_cv = Grid.predict(x_test)"
      ],
      "metadata": {
        "id": "9wS55w7PK2Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_xgb_cv_proba=Grid.predict_proba(x_test)"
      ],
      "metadata": {
        "id": "jDKevS9pgNS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling train_model to train,fit and evalution of extreme gradien boosting with grid search cv model\n",
        "train_model(Grid,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "93CJcKeoLV6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "mHlU_MAvkFN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_xgb_cv)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test, y_pred_xgb_cv )\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_xgb_cv ))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "TF_Iqx67L-SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC CURVE XGBoost_CV Model"
      ],
      "metadata": {
        "id": "AcSHAlC7PnGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "# y_pred_proba = gbboost.predict_proba(X_test)\n",
        "roc_auc_score(y_test, y_pred_xgb_cv_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_xgb_cv_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "ENWwF7BOgFWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Features as per XGBoost CV Model"
      ],
      "metadata": {
        "id": "uMo2zWUeS2dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = mobile_df.drop(columns=['price_range'], axis=1)\n",
        "Y = mobile_df['price_range']\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=10)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "  # calculate the feature importances\n",
        "features = X_train.columns\n",
        "importances = boosting.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()\n",
        "\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "print(importance_df.sort_values(by=['Feature Importance'],ascending=False))"
      ],
      "metadata": {
        "id": "6ceN6bWvTBmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the testing accuracy has slightly improved from 0.91 to 0.92."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 Decision Tree"
      ],
      "metadata": {
        "id": "ZfgdxsM8M935"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_dt = DecisionTreeClassifier( max_leaf_nodes=10, random_state=0)\n",
        "\n",
        "# Fit the Algorithm\n",
        "model_dt.fit(x_train,y_train)\n"
      ],
      "metadata": {
        "id": "nzHTjePjMPGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_train_dt=model_dt.predict(x_train)\n",
        "y_pred_test_dt=model_dt.predict(x_test)\n"
      ],
      "metadata": {
        "id": "5a6-UhBROK6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling train_model to train,fit and evalution of Decision tree model\n",
        "train_model(model_dt,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "xKhioIqOO6zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_dt)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test,y_pred_test_dt)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_test_dt ))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "IJcZKkiCTPJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree ROC Curve"
      ],
      "metadata": {
        "id": "oHItXgTgUXk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_pred_proba = model_dt.predict_proba(x_test)\n",
        "roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "cMCsbIx2VFsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 Support vector Machines"
      ],
      "metadata": {
        "id": "9N6nsH7RP-hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # build a svm model\n",
        "svm_model = SVC(decision_function_shape='ovo', probability=True)\n",
        "svm_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "LamrDEPHWmPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_train_svm=svm_model.predict(x_train)\n",
        "y_pred_test_svm=svm_model.predict(x_test)\n"
      ],
      "metadata": {
        "id": "jxx7c4gxSeVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling train_model to train,fit and evalution of SVM model\n",
        "train_model(svm_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "sF_BE_txWJ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_svm)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test,y_pred_test_svm)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_test_svm))"
      ],
      "metadata": {
        "id": "ysUJYYBJZkGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM ROC CURVE"
      ],
      "metadata": {
        "id": "G1VvtayZZ-cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_pred_proba = svm_model.predict_proba(x_test)\n",
        "roc_auc_score(y_test, y_pred_proba, multi_class='ovo', average='weighted')\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 4\n",
        "\n",
        "for i in range(n_class):\n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)\n",
        "\n",
        "# plotting multiclass ROC curve\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Low_cost')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Medium_cost')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='High_cost')\n",
        "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Very_high_cost')\n",
        "plt.title(' Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "7y8gearZZ8UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 KNN"
      ],
      "metadata": {
        "id": "ERZPUKw-bB3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "  # hyperparameters\n",
        "param_dict = {'n_neighbors': [3],\n",
        "                'leaf_size': [25]}\n",
        "\n",
        "  # build a xgboost model\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "  # random search\n",
        "knn_grid = RandomizedSearchCV(estimator=knn_model,\n",
        "                       param_distributions = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='accuracy')\n",
        "knn_grid.fit(x_train,y_train)\n",
        "\n",
        "knn_optimal_model = knn_grid.best_estimator_"
      ],
      "metadata": {
        "id": "yCkmn1m3bA3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_knn=knn_optimal_model.predict(x_train)\n",
        "y_pred_test_knn=knn_optimal_model.predict(x_test)"
      ],
      "metadata": {
        "id": "Y6BknxFJd9qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(knn_optimal_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "gQ-XFSFjeRER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # training confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for train set:\")\n",
        "matrix(y_train, y_pred_train_knn)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "  # testing confusion matrix\n",
        "print(\"\\n\")\n",
        "print(\"Confusion matrix for test set:\")\n",
        "matrix(y_test,y_pred_test_knn)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Classification report:\")\n",
        "print(classification_report(y_test,y_pred_test_knn))"
      ],
      "metadata": {
        "id": "iVnRqDcea1QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jchIbw9rcYiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dictionary to store all the metrices\n",
        "all_model_matrices={'Test_accuracy':Test_acc,'Test_precision':Test_precision,'Test_recall':Test_recall,'Test_f1_score':Test_f1, 'Test_roc':Test_roc}"
      ],
      "metadata": {
        "id": "75hInCQLbQvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list of all model made\n",
        "model_name=['Logistic Regression','Random Forest','Random Forest CV','XGBoost','XGBoost CV','Decision_Tree','SVM','K-Nearest_Neighbor']"
      ],
      "metadata": {
        "id": "ztJS6ZdCb2lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting dictionary to dataframe fro easy visual\n",
        "matrices_df=pd.DataFrame.from_dict(all_model_matrices,orient=\"index\",columns=model_name)"
      ],
      "metadata": {
        "id": "kQSioE6mb6P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrices_df"
      ],
      "metadata": {
        "id": "UJvmOqEOp4cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrices_df.T"
      ],
      "metadata": {
        "id": "mTnuHvciqbXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics are used to assess the performance of machine learning models.\n",
        "\n",
        " * Accuracy measures the proportion of correctly predicted instances out of the total instances. It indicates the overall correctness of the model's predictions. Higher accuracy generally suggests better performance. From a business perspective, accuracy helps in understanding the general reliability of the model's predictions. For example, in a spam email classification model, high accuracy means fewer legitimate emails are incorrectly classified as spam, reducing the risk of missing important messages. So here we have considered accuracy as a evaluation matrix for a positive business impact.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, I want to consider using the Xgboost_cv i.e., XGBClassifier with hyperparameter tunned model as final model. This model has the highest accuracy values on test sets, which indicates that it is doing a good job of explaining the variance in the target variable. which means that it is making relatively small and accurate predictions."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = mobile_df.drop(columns=['price_range'], axis=1)\n",
        "Y = mobile_df['price_range']\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=10)\n",
        "\n",
        "\n",
        "  # calculate the feature importances\n",
        "features = X_train.columns\n",
        "importances = boosting.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()\n",
        "\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "print(importance_df.sort_values(by=['Feature Importance'],ascending=False))"
      ],
      "metadata": {
        "id": "BdyNzorfgdME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We have reached the end of our mobile price range prediction project and have achieved a fairly good result for all the models implemented. We have discovered a lot of insights from the data through EDA which helps in determining which features will have a strong influence on the price range prediction.\n",
        "\n",
        "#### This kind of prediction is very important for businesses to understand which factors drive the price of a mobile phone and estimate the price of\n",
        "mobile phones accurately to give good competition to other manufacturers.\n",
        "\n",
        "#### Let us summarize the whole work in few points below:\n",
        "\n",
        "* The dataset contains 2000 records of mobile phone information with 21 features. There are 6 categorical features and 14 numerical features with price range as the target variable.\n",
        "\n",
        "* The dataset was almost a cleaned one with no null values present or duplicate records found.\n",
        "\n",
        "* EDA was used to generate insights or to get all sorts of information of the features in the dataset. With the help of EDA, we got to know that\n",
        "\n",
        "  * The target class data was mostly balanced with not much difference between each class.\n",
        "\n",
        "  * Most of the categorical features had a similar distribution or count except the feature 'three_g'. There were very few records for mobile phones which doesn't support 3G. We can infer that almost all phones had 3G network access if not 4G.\n",
        "\n",
        "  * Most of the numerical features follow an uniform distribution except few features like front camera, pixel resolution height and screen width which had a right skewed distribution.\n",
        "\n",
        "  * The boxplots suggested us that there are few outliers in two features but we know that these values are very natural in real life, thus we didn't treat them as outliers.\n",
        "\n",
        "  * Almost all categorical features had a similar distribution across all price ranges except 'three_g' where there were very few records of mobile phones not having 3G network access across all price ranges.\n",
        "\n",
        "  * There is a slight increase in count for each feature in the very high cost category except the feature touch screen.\n",
        "\n",
        "  * We can infer that the more we pay, more choices we get for mobile phones with all the features.\n",
        "\n",
        "  * Battery power and RAM increases with the increase in price range and thus these two features will be an influential factor for predicting the price range.\n",
        "\n",
        "  * Most of the other numerical features doesn't show a significant change with the price range.\n",
        "\n",
        "* Through correlation analysis we got to know that,\n",
        "\n",
        "  * RAM is the only feature which is strongly correlated with the target variable.\n",
        "\n",
        "  * Most of the other independent variables were not strongly correlated to each other and hence we didn't need to remove or combine any feature.\n",
        "\n",
        "  * Through correlation plots we have observed that except RAM, battery power, px_height and px_width are slightly correlated with the target variable and can be important features to determine the price ranges.\n",
        "\n",
        "  * No categorical feature is strongly correlated with the target variable and interestingly touch screen had a low negative correlation with the target variable.\n",
        "\n",
        "* We have implemented 6 classification models and have achieved a fairly good result for all the algorithms.\n",
        "\n",
        "  * Logistic Regression\n",
        "  * Random Forest\n",
        "  * XGBoosting\n",
        "  * Decision Tree\n",
        "  * K Nearest Neighbors\n",
        "  * Support Vector Machine\n",
        "\n",
        "* Logistic Regression and XGBoost has performed better than any other model by achieving an accuracy of 92% which is the highest among the 6 models implemented.\n",
        "\n",
        "* Through the tree based methods we found out that RAM, battery power, px_height and px_width are the most important features for the prediction of price ranges.\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}